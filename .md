# FarmLog STT→RAG Baseline (LangChain & DSPy)

영농일지(STT 텍스트) → RAG(내부/웹 지식) → 구조화 요약(JSON) → API 응답까지 한 번에 동작하는 최소 기능 제품(MVP)용 베이스라인입니다. LangChain 기반 파이프라인을 기본으로 제공하고, DSPy 대안도 함께 포함합니다.

---

## 1) 아키텍처 개요

```
[Whisper STT Text]
      │
      ▼
[Preprocess & Normalise]
      │
      ├─(선택) Web Search (DuckDuckGo/Google CSE)
      │
      ├─ RAG Retriever (Chroma + Embeddings)
      │       └─ KB: 농업/영농일지 템플릿, 병해충·시비 가이드, 작물별 관리 지침 등
      ▼
[LLM (GPT) with Persona + Template → Structured JSON]
      │
      ├─ Validation (Pydantic / JSON Schema)
      │
      ▼
[API Response / 저장(선택: DB, 파일)]
```

---

## 2) 폴더 구조(권장)

```
farmlog-baseline/
├─ .env.example
├─ requirements.txt
├─ kb/                           # 내부 지식(로컬 문서, .md/.txt 권장)
│   ├─ 농약_안전사용_가이드.txt
│   ├─ 영농일지_모범_템플릿.md
│   └─ 작물별_관리요령_요약.txt
├─ chroma/                       # 벡터스토어 영속화 폴더(자동 생성)
└─ src/
   ├─ prompts.py
   ├─ search.py
   ├─ rag.py
   ├─ pipeline_langchain.py
   ├─ dsp_baseline.py
   └─ app_fastapi.py
```

---

## 3) `.env.example`

```ini
# OpenAI
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBED_MODEL=text-embedding-3-small

# Search (옵션)
# DUCKDUCKGO는 키 불필요. Google CSE 사용 시 아래 2개 필요
GOOGLE_CSE_ID=
GOOGLE_API_KEY=

# 파이프라인 옵션
USE_WEB_SEARCH=0          # 1이면 DuckDuckGo 검색 사용
RETRIEVE_TOP_K=4
CHROMA_DIR=./chroma
KB_DIR=./kb
```

---

## 4) `requirements.txt`

```txt
fastapi
uvicorn[standard]
python-dotenv
pydantic>=2
langchain>=0.2.12
langchain-openai
langchain-community
chromadb
sentence-transformers
duckduckgo-search
tiktoken
typing_extensions
# DSPy (옵션)
dspy-ai
```

---

## 5) `src/prompts.py`

```python
# -*- coding: utf-8 -*-
"""
영농일지용 페르소나 & 시스템 지시문 템플릿
- 구조화 JSON 출력(필드 스키마는 pipeline_langchain.FarmLog 참고)
"""

AGRI_PERSONA = (
    """
당신은 농업 컨설턴트이자 품질관리 담당자입니다.
역할:
- STT로 전사된 농작업 메모를 읽고 누락·모호 표현을 정리합니다.
- 작물·포장·날씨·작업·투입자재·문제/이슈·다음 조치 등을 구조화합니다.
- 사실과 추론을 구분하고, 추정치는 근거를 덧붙입니다.
- 안전(농약 PPE, 재진입 간격)과 법규 준수를 우선합니다.
출력은 반드시 **한국어**이며, 지정된 JSON 스키마만을 반환합니다.
    """
)

SYSTEM_INSTRUCTIONS = (
    """
지침:
1) 입력 STT 텍스트의 구어체·중복·비문을 정리하되 의미를 바꾸지 않습니다.
2) RAG 문서와 웹 검색 노트의 지식은 참고로만 사용하고, 원문(STT) 우선으로 기록합니다.
3) 단위(kg, L, mm, 인분량 등)와 수치가 없으면 null 또는 추정치+근거를 기입합니다.
4) 날짜·시간·위치는 입력/메타에 없으면 오늘 날짜/미상으로 둡니다.
5) 출력 JSON 스키마 외 문자는 금지합니다(설명, 마크다운 금지).
    """
)

USER_TEMPLATE = (
    """
[입력 STT]
{stt_text}

[메타데이터]
- date_hint: {date_hint}
- crop_hint: {crop_hint}
- location_hint: {location_hint}

[컨텍스트]
- RAG 문서 요약: {rag_context}
- 웹 검색 노트: {web_notes}
    """
)
```

---

## 6) `src/search.py`

```python
# -*- coding: utf-8 -*-
"""DuckDuckGo(무키) 또는 Google CSE(유료키) 웹 검색 래퍼"""
from typing import List, Dict, Optional
import os

# DuckDuckGo (no key)
try:
    from duckduckgo_search import DDGS  # pip install duckduckgo-search
    _HAS_DDG = True
except Exception:
    _HAS_DDG = False

# Google CSE (optional)
try:
    from googleapiclient.discovery import build  # pip install google-api-python-client
    _HAS_GCSE = True
except Exception:
    _HAS_GCSE = False


def ddg_search(query: str, max_results: int = 5) -> List[Dict]:
    if not _HAS_DDG:
        return []
    try:
        with DDGS() as ddg:
            hits = list(ddg.text(query, max_results=max_results))
        # 표준화
        return [
            {
                "title": h.get("title"),
                "href": h.get("href") or h.get("link"),
                "body": h.get("body") or h.get("snippet")
            }
            for h in hits
        ]
    except Exception:
        return []


def google_cse_search(query: str, max_results: int = 5) -> List[Dict]:
    cx = os.getenv("GOOGLE_CSE_ID")
    key = os.getenv("GOOGLE_API_KEY")
    if not (_HAS_GCSE and cx and key):
        return []
    try:
        service = build("customsearch", "v1", developerKey=key)
        res = service.cse().list(q=query, cx=cx, num=max_results).execute()
        items = res.get("items", [])
        return [
            {
                "title": it.get("title"),
                "href": it.get("link"),
                "body": it.get("snippet")
            }
            for it in items
        ]
    except Exception:
        return []


def web_search_notes(queries: List[str], max_per_query: int = 3) -> str:
    """여러 쿼리를 순회하며 요약형 노트 문자열 구성"""
    if not queries:
        return ""
    notes = []
    use_web = os.getenv("USE_WEB_SEARCH", "0").strip() == "1"
    if not use_web:
        return ""
    for q in queries:
        hits = ddg_search(q, max_per_query) or google_cse_search(q, max_per_query)
        if not hits:
            continue
        notes.append(f"[검색: {q}]")
        for h in hits:
            notes.append(f"- {h['title']} — {h['href']} :: {h['body']}")
    return "\n".join(notes[:1000])  # 노트 길이 제한
```

---

## 7) `src/rag.py`

```python
# -*- coding: utf-8 -*-
"""로컬 KB 인덱싱 및 Retriever"""
from typing import List, Optional
import os, glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


def _load_kb_texts(kb_dir: str) -> List[str]:
    exts = ("*.txt", "*.md")
    files = []
    for ext in exts:
        files.extend(glob.glob(os.path.join(kb_dir, ext)))
    texts = []
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                texts.append(f.read())
        except Exception:
            pass
    return texts


def build_or_load_vectorstore(kb_dir: str, persist_dir: str) -> Chroma:
    os.makedirs(persist_dir, exist_ok=True)
    embed_model = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
    embeddings = OpenAIEmbeddings(model=embed_model)

    # 이미 존재하면 로드
    if os.path.exists(os.path.join(persist_dir, "chroma.sqlite3")):
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)

    # 새로 빌드
    raw_texts = _load_kb_texts(kb_dir)
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    docs = splitter.create_documents(raw_texts)
    vs = Chroma.from_documents(docs, embedding=embeddings, persist_directory=persist_dir)
    vs.persist()
    return vs


def get_retriever(vs: Chroma, k: int = 4):
    return vs.as_retriever(search_kwargs={"k": k})
```

---

## 8) `src/pipeline_langchain.py`

```python
# -*- coding: utf-8 -*-
from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os

from .prompts import AGRI_PERSONA, SYSTEM_INSTRUCTIONS, USER_TEMPLATE
from .rag import build_or_load_vectorstore, get_retriever
from .search import web_search_notes

# ---------- 출력 스키마 ----------
class Operation(BaseModel):
    kind: str = Field(description="작업 종류 (파종/정식/관수/시비/방제/제초/수확/기타)")
    description: Optional[str] = None
    quantity: Optional[float] = None
    unit: Optional[str] = None

class Issue(BaseModel):
    title: str
    details: Optional[str] = None
    severity: Optional[str] = Field(default=None, description="low/medium/high")

class NextAction(BaseModel):
    action: str
    due_date: Optional[str] = None  # YYYY-MM-DD

class FarmLog(BaseModel):
    date: Optional[str] = None
    farmer: Optional[str] = None
    location: Optional[str] = None
    crop: Optional[str] = None
    weather: Optional[str] = None
    operations: List[Operation] = []
    issues: List[Issue] = []
    notes: Optional[str] = None
    next_actions: List[NextAction] = []
    references: List[str] = []  # RAG/웹 근거 링크 또는 문서명


# ---------- 파이프라인 ----------
class FarmLogPipeline:
    def __init__(self):
        load_dotenv()
        model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.llm = ChatOpenAI(model=model_name, temperature=0)
        self.vs = build_or_load_vectorstore(
            kb_dir=os.getenv("KB_DIR", "./kb"),
            persist_dir=os.getenv("CHROMA_DIR", "./chroma"),
        )
        self.retriever = get_retriever(self.vs, k=int(os.getenv("RETRIEVE_TOP_K", "4")))
        self.structured_llm = self.llm.with_structured_output(FarmLog)

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", AGRI_PERSONA + "\n" + SYSTEM_INSTRUCTIONS),
            ("user", USER_TEMPLATE),
        ])

    def _clean_stt(self, text: str) -> str:
        text = text.replace("\n", " ")
        while "  " in text:
            text = text.replace("  ", " ")
        return text.strip()

    def _rag_context(self, query_hint: Optional[str], stt_text: str) -> str:
        q = query_hint or (stt_text[:80] if stt_text else "영농일지")
        docs = self.retriever.invoke(q)
        ctx = []
        refs = []
        for d in docs:
            page = getattr(d, "page_content", "")
            meta = getattr(d, "metadata", {})
            if page:
                ctx.append(page[:600])
            if meta and meta.get("source"):
                refs.append(str(meta["source"]))
        return "\n\n".join(ctx), refs

    def run(self,
            stt_text: str,
            date_hint: Optional[str] = None,
            crop_hint: Optional[str] = None,
            location_hint: Optional[str] = None,
            search_queries: Optional[List[str]] = None) -> FarmLog:
        stt = self._clean_stt(stt_text)
        rag_ctx, refs = self._rag_context(crop_hint or "영농", stt)
        web_notes = web_search_notes(search_queries or [])

        filled = self.prompt.invoke({
            "stt_text": stt,
            "date_hint": date_hint or "",
            "crop_hint": crop_hint or "",
            "location_hint": location_hint or "",
            "rag_context": rag_ctx,
            "web_notes": web_notes or "",
        })
        result: FarmLog = self.structured_llm.invoke(filled)

        # 참고 링크 병합
        if web_notes:
            links = [ln.split(" — ")[1].split(" :: ")[0] for ln in web_notes.splitlines() if ln.startswith("-") and " — " in ln]
            result.references.extend(links)
        if refs:
            result.references.extend(refs)
        # 중복 제거
        result.references = sorted(set([r for r in result.references if r]))
        return result
```

---

## 9) `src/dsp_baseline.py` (선택)

```python
# -*- coding: utf-8 -*-
"""DSPy를 이용한 간단한 구조화 요약 파이프라인(웹 검색/벡터 검색은 외부에서 준비해 컨텍스트로 주입)"""
from typing import Optional, List
from pydantic import BaseModel
import dspy
from dotenv import load_dotenv

class FarmLogFields(BaseModel):
    json: str  # LLM이 JSON 문자열만 출력하도록 강제

class MakeFarmLog(dspy.Signature):
    """STT 텍스트와 보조 컨텍스트(RAG, 웹노트)를 읽고 영농일지 JSON을 만듭니다.
    출력은 반드시 유효한 JSON 문자열이어야 합니다.
    """
    stt: str = dspy.InputField()
    context: str = dspy.InputField()
    answer: FarmLogFields = dspy.OutputField()

class DSPyFarmLog:
    def __init__(self, model_name: str = "gpt-4o-mini"):
        load_dotenv()
        lm = dspy.OpenAI(model=model_name, temperature=0)
        dspy.settings.configure(lm=lm)
        self.predictor = dspy.Predict(MakeFarmLog)

    def run(self, stt: str, context: str) -> str:
        out = self.predictor(stt=stt, context=context)
        return out.answer.json
```

---

## 10) `src/app_fastapi.py`

```python
# -*- coding: utf-8 -*-
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv
import os

from .pipeline_langchain import FarmLogPipeline, FarmLog
from .rag import build_or_load_vectorstore

app = FastAPI(title="FarmLog STT→RAG Baseline")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SummariseRequest(BaseModel):
    stt_text: str
    date_hint: Optional[str] = None
    crop_hint: Optional[str] = None
    location_hint: Optional[str] = None
    search_queries: Optional[List[str]] = None

class IngestRequest(BaseModel):
    kb_dir: Optional[str] = None

# lazy init
_pipeline: Optional[FarmLogPipeline] = None

@app.on_event("startup")
def _startup():
    load_dotenv()
    # 선행 인덱싱 (없으면 빌드)
    build_or_load_vectorstore(
        kb_dir=os.getenv("KB_DIR", "./kb"),
        persist_dir=os.getenv("CHROMA_DIR", "./chroma"),
    )

@app.post("/summarise", response_model=FarmLog)
def summarise(req: SummariseRequest):
    global _pipeline
    if _pipeline is None:
        _pipeline = FarmLogPipeline()
    return _pipeline.run(
        stt_text=req.stt_text,
        date_hint=req.date_hint,
        crop_hint=req.crop_hint,
        location_hint=req.location_hint,
        search_queries=req.search_queries,
    )

@app.post("/ingest")
def ingest(req: IngestRequest):
    # KB 폴더가 바뀐 경우 강제로 재인덱싱하고 파이프라인 리셋
    kb_dir = req.kb_dir or os.getenv("KB_DIR", "./kb")
    vs = build_or_load_vectorstore(kb_dir=kb_dir, persist_dir=os.getenv("CHROMA_DIR", "./chroma"))
    global _pipeline
    _pipeline = None
    return {"status": "ok", "kb_dir": kb_dir}

@app.get("/healthz")
def healthz():
    return {"status": "ok"}
```

---

## 11) 빠른 시작

```bash
# 1) 가상환경 & 설치
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2) 환경변수 준비
cp .env.example .env
# OPENAI_API_KEY 등 채우기

# 3) KB 문서 추가
mkdir -p kb
# 여기에 .txt/.md 파일 추가(작물별 지침, 농약안전, 템플릿 등)

# 4) API 실행
uvicorn src.app_fastapi:app --host 0.0.0.0 --port 8001

# 5) 테스트
curl -X POST http://localhost:8001/summarise \
  -H 'Content-Type: application/json' \
  -d '{
        "stt_text": "오늘 오전 9시에 배추 밭 2번 포장에서 관수하고 요소 2kg 물 100L에 타서 엽면시비. 진딧물 약간 관찰.",
        "date_hint": "2025-09-22",
        "crop_hint": "배추",
        "location_hint": "포장-2",
        "search_queries": ["배추 진딧물 방제 요령", "요소 엽면시비 농도"]
      }'
```

---

## 12) 운영 팁

* **웹 검색 제어**: `.env`의 `USE_WEB_SEARCH=1`로 활성화. (DuckDuckGo 기본, Google CSE 키 제공 시 우선)
* **스키마 확장**: `pipeline_langchain.FarmLog`에 필드 추가 → 자동 검증·구조화.
* **지식 업데이트**: `kb/`에 문서 추가 후 `/ingest` 호출.
* **DB 연동(선택)**: FastAPI 라우트에서 결과 JSON을 RDBMS/NoSQL에 저장하도록 확장.
* **출력 일관성**: LangChain의 `with_structured_output`로 JSON 형태 강제.
* **성능**: Chroma는 기본 설정으로 충분. 대규모는 Weaviate/FAISS로 교체 가능.

---

## 13) 다음 단계(선택)

* 날씨 API(예: KMA/기상청 또는 Open-Meteo)로 당일 강수·기온 자동 주석 추가
* 음성화(합성)로 결과 읽어주기, 모바일 입력 최적화
* 역할 기반 프롬프트(소규모/대규모 농장, 유기/관행 등) 프로필화
* 규정 준수 체크리스트 자동 생성(농약 R.E.I., PHI 등)
